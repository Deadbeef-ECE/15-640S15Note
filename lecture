Jan/22 Lecture 4
End-to-End Reasoning

(1)RPC with TCP & UDP
(2)Large File Transfer
chunk:recovery(TCP feature)

Make DS faster:
(1)delay: processing delay, transimission delay(don't care), queueing delay(dominate)

Queueing Theory:
Properties(consider AT MOST 2 OUT of 3):
(1) Efficiency(resource utilization); 
(2) Crispness(response time); 
(3) Freedom(no advance reservations, just use when needed).

Latency and Bandwith:
delay-Bandwith product(abort the request in the midway).

Latency(killer, not bandwith):hard to improve
Bandwith(improved through parallelism)

Many software and systems trends to increase the Latency

Impact of Long Latency:
synchronous model of RPC become infeasible

==============================================================================

Jan/26 Lecture 5
Caching
Then only technique to reduce end-to-end latency
Local storage used for copies is referred to as cache

Simple Cache Metrics
References, Hits, Misses
Miss Ration = Misses/References
Hit Ratio = Hits/References = (1 - Miss Ration)
Expected cost of a reference = (Miss Ration * cost of miss) + (Hit Ration * cost of hit)
Cache Advantage = (Cost of Miss/Cost of Hit)

Key Question:
(1) Fetch policy(when & why)
(2) Update propagation policy
(3) Cache replacement policy

Fetch Policy:
Approach 1: Full Replication(dropbox)
Every participating machine gets a full and complete copy.
All date is fetched in advance.
Shortcomings: Coarse-grain, non-selective management of data

Approach 2: on-demand caching(AFS)
Fine-grained and selective approach to data management.
- OS modification
+ total application transparency
+ enable demand caching

=============================================================================

Jan/29 Lecture 6
on-demand Caching

CDN(Content Distribution Network): 3rd-party caching
IC: Intrinsic Content(cacheable)
PC: Parasitic Content(IC+PC uncacheable)

**edge caching

Key Question:
(1) Fetch Policy
(2) Update propagation Policy
(3) Cahce Replacement Policy

Cost: (1/Speed)*size + Latency
Spatial locality(of reference)
Remote data more coarsely addressable than local.

FETCH POLICY:
    Combination: fetch more than need on miss
    Advantage:
    (1) amortize the cost;
    (2) assumption sometimes violated
    Disadvantage:
    (1) Useless content may hurt.

    Spatial & Temporal locality
    caching tightly combines these assumptions;
    one can exist without the ohter;

UPDATE PROPAGATION POLICY:
One-copy semantics
    Cache Consistency:
    why hard:
    (1) Physical master copy may not exist
    (2) Network break(users-> master copy)
    (3) Intense read and write sharing across sites(increase traffic)

    Cache from the cloud(Center and Edge)

    1.Broadcast Invalidations:
    Notify every caching site on every update.
    
    2. Check on use:
    reader checks msater copy before each use(waste a lot of time);

session semantics
    check when access entire file or large block;
    (applying check only when file is open:upadte and the changes each time) 
    Advantage & Disadvantage(See slides)
    
==================================================================================
Feb/3  Lecture 7















