Jan/22 Lecture4
End-to-End Reasoning

(1)RPC with TCP & UDP
(2)Large File Transfer
chunk:recovery(TCP feature)

Make DS faster:
(1)delay: processing delay, transimission delay(don't care), queueing delay(dominate)

Queueing Theory:
Properties(consider AT MOST 2 OUT of 3):
(1) Efficiency(resource utilization); 
(2) Crispness(response time); 
(3) Freedom(no advance reservations, just use when needed).

Latency and Bandwith:
delay-Bandwith product(abort the request in the midway).

Latency(killer, not bandwith):hard to improve
Bandwith(improved through parallelism)

Many software and systems trends to increase the Latency

Impact of Long Latency:
synchronous model of RPC become infeasible

Jan/26 Lecture4
Caching
Then only technique to reduce end-to-end latency
Local storage used for copies is referred to as cache

Simple Cache Metrics
References, Hits, Misses
Miss Ration = Misses/References
Hit Ratio = Hits/References = (1 - Miss Ration)
Expected cost of a reference = (Miss Ration * cost of miss) + (Hit Ration * cost of hit)
Cache Advantage = (Cost of Miss/Cost of Hit)

Key Question:
(1) Fetch policy(when & why)
(2) Update propagation policy
(3) Cache replacement policy

Fetch Policy:
Approach 1: Full Replication(dropbox)
Every participating machine gets a full and complete copy.
All date is fetched in advance.
Shortcomings: Coarse-grain, non-selective management of data

Approach 2: on-demand caching(AFS)
Fine-grained and selective approach to data management.
- OS modification
+ total application transparency
+ enable demand caching


